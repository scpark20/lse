{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de1a9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from vae_helpers import get_1x1, get_3x3\n",
    "from diffusers import DDPMScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "65d29ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "    ).to(device=timesteps.device)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding\n",
    "\n",
    "def parse_layer_string(s):\n",
    "    layers = []\n",
    "    for ss in s.split(','):\n",
    "        if 'x' in ss:\n",
    "            res, num = ss.split('x')\n",
    "            count = int(num)\n",
    "            layers += [(int(res), None) for _ in range(count)]\n",
    "        elif 'm' in ss:\n",
    "            res, mixin = [int(a) for a in ss.split('m')]\n",
    "            layers.append((res, mixin))\n",
    "        elif 'd' in ss:\n",
    "            res, down_rate = [int(a) for a in ss.split('d')]\n",
    "            layers.append((res, down_rate))\n",
    "        else:\n",
    "            res = int(ss)\n",
    "            layers.append((res, None))\n",
    "    return layers\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_width, middle_width, out_width, residual=False, use_3x3=True, zero_last=False, time_width=0):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        self.c1 = get_1x1(in_width, middle_width)\n",
    "        self.c2 = get_3x3(middle_width, middle_width) if use_3x3 else get_1x1(middle_width, middle_width)\n",
    "        self.c3 = get_3x3(middle_width, middle_width) if use_3x3 else get_1x1(middle_width, middle_width)\n",
    "        self.c4 = get_1x1(middle_width, out_width, zero_weights=zero_last)\n",
    "        self.time_width = time_width\n",
    "        if time_width > 0:\n",
    "            self.time_embed = get_1x1(time_width, middle_width)\n",
    "\n",
    "    def forward(self, x, t=None):\n",
    "        if t is None:\n",
    "            t = 0\n",
    "        else:\n",
    "            t = timestep_embedding(t, self.time_width)[:, :, None, None]\n",
    "            t = self.time_embed(t)\n",
    "            \n",
    "        xhat = self.c1(F.gelu(x))\n",
    "        xhat = self.c2(F.gelu(xhat) + t)\n",
    "        xhat = self.c3(F.gelu(xhat) + t)\n",
    "        xhat = self.c4(F.gelu(xhat))\n",
    "        out = x + xhat if self.residual else xhat\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6f66a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusion(nn.Module):\n",
    "    def __init__(self, H, **kwargs):\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.scheduler = H.scheduler\n",
    "        self.backbones = nn.ModuleList()\n",
    "        blocks = parse_layer_string(H.dec_blocks)\n",
    "        for idx, (res, mixin) in enumerate(blocks):\n",
    "            block = Block(H.zdim, H.diff_middle_width, H.zdim,\n",
    "                          H.diff_residual, True, True, H.zdim)\n",
    "            self.backbones.append(block)\n",
    "        \n",
    "    def forward(self, data, **kwargs):\n",
    "        # data['stats'] : [{'pm','pv','z'}, ...]\n",
    "        \n",
    "        losses = 0\n",
    "        for stat, backbone in zip(data['stats'], self.backbones):\n",
    "            z0 = (stat['z'] - stat['pm']) / torch.exp(stat['pv'])\n",
    "            e = torch.randn_like(z0)\n",
    "            t = torch.randint(0, self.scheduler.config.num_train_timesteps, size=(len(z0),)).to(z0.device)\n",
    "            zt = scheduler.add_noise(z0, e, t)\n",
    "            pred = backbone(zt, t)\n",
    "            if self.scheduler.config.prediction_type == 'epsilon':\n",
    "                loss = F.mse_loss(pred, e)\n",
    "            elif self.scheduler.config.prediction_type == 'sample':\n",
    "                loss = F.mse_loss(pred, z0)\n",
    "            losses += loss\n",
    "        \n",
    "        data['diffusion_loss'] = losses / len(self.backbones)\n",
    "        return data\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e2031a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "hp = EasyDict()\n",
    "\n",
    "hp.scheduler = DDPMScheduler()\n",
    "hp.zdim = 16\n",
    "hp.diff_middle_width = 384\n",
    "hp.diff_residual = True\n",
    "hp.dec_blocks = \"1x2,4m1,4x3,8m4,8x4,16m8,16x9,32m16,32x21,64m32,64x13,128m64,128x7,256m128\"\n",
    "\n",
    "model = LatentDiffusion(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "97c69ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['stats', 'diffusion_loss'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'stats': [{'pm': torch.randn(2, hp.zdim, 8, 8),\n",
    "                   'pv': torch.randn(2, hp.zdim, 8, 8),\n",
    "                   'z': torch.randn(2, hp.zdim, 8, 8)\n",
    "                  } for _ in range(len(parse_layer_string(hp.dec_blocks)))\n",
    "                 ]\n",
    "       }\n",
    "data = model(data)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d08a37fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0452, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['diffusion_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db590bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f26138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4342f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861db95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ste",
   "language": "python",
   "name": "ste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
