{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cf5ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Bottle(nn.Module):\n",
    "    def __init__(self, K, latent_channels):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.latent_channels = latent_channels\n",
    "        self.codebook_sum = None\n",
    "        self.codebook_elem = None\n",
    "        self.register_buffer('codebook', torch.zeros(self.K, self.latent_channels))\n",
    "        self.threshold = 1.0\n",
    "        self.mu = 0.99\n",
    "        self.register_buffer('init', torch.zeros(1))\n",
    "                \n",
    "    def _quantize(self, ze):\n",
    "        # ze : (n, c)\n",
    "        \n",
    "        # (n, K)\n",
    "        distance = (ze**2).sum(-1, keepdim=True) -\\\n",
    "                   2*ze@self.codebook.T +\\\n",
    "                   (self.codebook.T**2).sum(0, keepdim=True)\n",
    "        # (n,), (n,)\n",
    "        min_distance, zi = torch.min(distance, dim=-1)\n",
    "        return zi\n",
    "    \n",
    "    def _dequantize(self, zi):\n",
    "        # zi : (n,)\n",
    "        # (n, c)\n",
    "        zq = F.embedding(zi, self.codebook)\n",
    "        return zq\n",
    "    \n",
    "    def _tile(self, x):\n",
    "        d, ew = x.shape\n",
    "        if d < self.k_bins:\n",
    "            n_repeats = (self.k_bins + d - 1) // d\n",
    "            std = 0.01 / np.sqrt(ew)\n",
    "            x = x.repeat(n_repeats, 1)\n",
    "            x = x + t.randn_like(x) * std\n",
    "        return x\n",
    "    \n",
    "    # Choose K vectors from the data with additional noise\n",
    "    def _get_codebook_from_data(self, ze):\n",
    "        # ze: (K, c)\n",
    "        \n",
    "        codebooks = []\n",
    "        k = 0\n",
    "        while True:\n",
    "            codebook = ze[torch.randperm(ze.shape[0])][:self.K]\n",
    "            std = 0.01 / np.sqrt(self.latent_channels)\n",
    "            codebook = codebook + torch.randn_like(codebook) * std\n",
    "            codebooks.append(codebook)\n",
    "            k += len(codebook)\n",
    "            if k >= self.K:\n",
    "                break\n",
    "        codebook = torch.cat(codebooks, dim=0)[:self.K]\n",
    "        return codebook\n",
    "    \n",
    "    def _initialize(self, ze):\n",
    "        # ze : (n, c)\n",
    "        \n",
    "        self.codebook = self._get_codebook_from_data(ze)\n",
    "        self.codebook_sum = self.codebook.clone()\n",
    "        self.codebook_elem = torch.ones(self.K, device=self.codebook.device)\n",
    "        \n",
    "    def _update(self, ze, zi):\n",
    "        # ze : (n, c)\n",
    "        # zi : (n,)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            '''Calculate current centroids of the z embeddings = codebook_sum/codebook_elem'''\n",
    "            # (n, K)\n",
    "            zi_onehot = F.one_hot(zi, num_classes=self.K).float()\n",
    "            # (K, c) = (K, n) @ (n, c)\n",
    "            codebook_sum_current = zi_onehot.T @ ze\n",
    "            # (K,)\n",
    "            codebook_elem_current = zi_onehot.sum(0)\n",
    "\n",
    "            '''Obtain randomly a new centroids for bins whose usage is lower than the threshold'''\n",
    "            # (K, c)\n",
    "            codebook_random = self._get_codebook_from_data(ze)\n",
    "            \n",
    "            if self.init and self.codebook_sum is None:\n",
    "                self.codebook_sum = self.codebook.clone()\n",
    "                self.codebook_elem = torch.ones(self.K, device=self.codebook.device)\n",
    "\n",
    "            '''Update current centroids parameters'''\n",
    "            self.codebook_sum = self.mu*self.codebook_sum + (1.-self.mu)*codebook_sum_current\n",
    "            self.codebook_elem = self.mu*self.codebook_elem + (1.-self.mu)*codebook_elem_current\n",
    "\n",
    "            '''Update centroids'''\n",
    "            # (K, 1)\n",
    "            usage = (self.codebook_elem.unsqueeze(1) >= self.threshold).float()\n",
    "            codebook_prob = self.codebook_elem / self.codebook_elem.sum()\n",
    "            entropy = -torch.sum(codebook_prob*torch.log(codebook_prob + 1e-8))\n",
    "            # (K, c)\n",
    "            codebook_new = self.codebook_sum / self.codebook_elem.unsqueeze(1)\n",
    "            # (k, c)\n",
    "            self.codebook = usage*codebook_new + (1-usage)*codebook_random\n",
    "            outputs = {'usage': usage.sum(),\n",
    "                       'entropy': entropy}\n",
    "            return outputs\n",
    "        \n",
    "    def forward(self, ze, q_level=1.0):\n",
    "        # ze : (b, c)\n",
    "                \n",
    "        if not self.init:\n",
    "            self.init.data.fill_(1.)\n",
    "            self._initialize(ze)\n",
    "                        \n",
    "        # z_index : (b,)\n",
    "        zi = self._quantize(ze)\n",
    "        # z_quantized : (b, c)\n",
    "        zq = self._dequantize(zi)\n",
    "        # update codebook\n",
    "        update_outputs = self._update(ze, zi)\n",
    "        # Commitment loss\n",
    "        commit_loss = F.mse_loss(ze, zq)\n",
    "        # pass-through\n",
    "        zq = ze + (zq - ze).detach() * q_level\n",
    "        outputs = {'commit_loss': commit_loss,\n",
    "                   'zi': zi,\n",
    "                   'zq': zq}\n",
    "        outputs.update(update_outputs)\n",
    "        return outputs\n",
    "    \n",
    "class Latent(nn.Module):\n",
    "    def __init__(self, n_latents, z_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.bottles = nn.ModuleList([Bottle(2, z_dim) for _ in range(n_latents)])\n",
    "        self.linear = nn.Linear(z_dim, n_latents*z_dim)\n",
    "                \n",
    "    def forward(self, data, **kwargs):\n",
    "        # data['z'] : (N, z, H, W)\n",
    "        \n",
    "        N, z_dim, H, W = data['z'].size()\n",
    "        # (NHW, z)\n",
    "        z = data['z'].permute(0, 2, 3, 1).reshape(N*H*W, z_dim)\n",
    "        # (BHW, L, z)\n",
    "        z = self.linear(z).reshape(-1, len(self.bottles), z_dim)\n",
    "        commit_loss = 0\n",
    "        zq = []\n",
    "        for l in range(len(self.bottles)):\n",
    "            outputs = self.bottles[l](z[:, l])\n",
    "            commit_loss += outputs['commit_loss']\n",
    "            zq.append(outputs['zq'])\n",
    "        data['commit_loss'] = commit_loss\n",
    "        # (NHW, z_dim)\n",
    "        zq = torch.sum(zq, dim=1)\n",
    "        # (N, z, H, W)\n",
    "        data['z'] = zq.reshape(N, H, W, z_dim).permute(0, 3, 1, 2)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1c9deb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (list, dim=int), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m)}\n\u001b[0;32m----> 2\u001b[0m \u001b[43mLatent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scpark/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[20], line 152\u001b[0m, in \u001b[0;36mLatent.forward\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommit_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m commit_loss\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# (NHW, z_dim)\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m zq \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# (N, z, H, W)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m zq\u001b[38;5;241m.\u001b[39mreshape(N, H, W, z_dim)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (list, dim=int), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "data = {'z': torch.randn(2, 512, 16, 16)}\n",
    "Latent(9, 512)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5d2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d931b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef32f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
