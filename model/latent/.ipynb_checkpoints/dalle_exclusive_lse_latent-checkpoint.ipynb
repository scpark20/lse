{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d720d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Latent(nn.Module):\n",
    "    def __init__(self, init_log_sigma, const_sigma, **kwargs):\n",
    "        super().__init__()\n",
    "        self.log_sigma = nn.Parameter(torch.ones(1) * init_log_sigma, requires_grad=not const_sigma)\n",
    "                \n",
    "    def forward(self, data, **kwargs):\n",
    "        # data['z'] : (N, c, H, W)\n",
    "        # data['e'] : (M, c)\n",
    "        \n",
    "        z_dim = data['z'].shape[1]\n",
    "        # (NHW, c)\n",
    "        z = data['z'].permute(0, 2, 3, 1).reshape(-1, z_dim)\n",
    "        N = len(z)\n",
    "        T = kwargs['latent_temp'] if 'latent_temp' in kwargs else 1.0\n",
    "        softmax_temp = kwargs['softmax_temp'] if 'softmax_temp' in kwargs else 1.0\n",
    "        \n",
    "        # (NHW, M) = sum((NHW, 1, z) - (1, M, z), dim=2)\n",
    "        distance = torch.norm(z.unsqueeze(1) - data['e'].unsqueeze(0), dim=2) ** 2\n",
    "        alpha = -1/(2*torch.exp(self.log_sigma)**2)\n",
    "        matrix = alpha*distance/T\n",
    "        data['matrix'] = matrix\n",
    "        # (NHW, M)\n",
    "        belong = data['belong'] if 'belong' in data else None\n",
    "        loss = -torch.mean(T*CustomLogSumExp.apply(matrix, belong, softmax_temp))\n",
    "        loss = loss + 0.5*z_dim*(2*self.log_sigma-np.log(np.e)) + np.log(N)        \n",
    "        data['lse_loss'] = loss\n",
    "        \n",
    "        return data\n",
    "    \n",
    "class CustomLogSumExp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, belong=None, temp=1):\n",
    "        # input : (N, M)\n",
    "        # belong : (N, M)\n",
    "        \n",
    "        ctx.temp = temp\n",
    "        # (1, M)\n",
    "        output = torch.logsumexp(input, dim=0, keepdim=True)\n",
    "        ctx.save_for_backward(input, output, belong)\n",
    "        return output.squeeze(0)  # output을 반환할 때는 차원을 줄입니다.\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        temp = ctx.temp\n",
    "        # (N, M), (1, M), (N, M)\n",
    "        input, output, belong = ctx.saved_tensors\n",
    "        # softmax 함수를 사용하여 그래디언트 계산을 수행합니다.\n",
    "        # (N, M)\n",
    "        if belong is None:\n",
    "            softmax_result = torch.exp(input - output)\n",
    "        else:\n",
    "            inner_value = belong * input + (1-belong) * (np.log(max(temp, 1e-15)) + input)\n",
    "            softmax_result = torch.softmax(inner_value, dim=0)\n",
    "        grad_input = softmax_result * grad_output.unsqueeze(0)\n",
    "        return grad_input, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3809e700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'z': tensor([[[[ 0.3048, -1.0783],\n",
       "           [ 0.6670,  0.8956]]],\n",
       " \n",
       " \n",
       "         [[[ 0.4635, -0.5543],\n",
       "           [-0.9337,  0.7969]]],\n",
       " \n",
       " \n",
       "         [[[-2.0398,  1.2159],\n",
       "           [ 0.2800, -2.3699]]],\n",
       " \n",
       " \n",
       "         [[[-1.1778, -1.7241],\n",
       "           [-2.3137,  0.8922]]]]),\n",
       " 'e': tensor([[ 1.9999],\n",
       "         [-0.1768],\n",
       "         [-0.3227],\n",
       "         [ 0.4121],\n",
       "         [ 0.0367],\n",
       "         [ 0.7077],\n",
       "         [-1.1629],\n",
       "         [ 0.6970],\n",
       "         [-1.0937],\n",
       "         [ 1.7306]]),\n",
       " 'belong': tensor([[0, 0, 0, 0, 1, 0, 1, 0, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 0, 1, 0, 1, 1],\n",
       "         [1, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 1, 0, 1, 1, 1, 0],\n",
       "         [0, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
       "         [0, 0, 1, 0, 1, 0, 0, 1, 1, 1],\n",
       "         [0, 1, 1, 0, 1, 0, 1, 0, 1, 1],\n",
       "         [1, 0, 1, 0, 1, 1, 0, 1, 1, 1],\n",
       "         [1, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 1, 1, 1, 0, 1, 1, 1],\n",
       "         [1, 0, 0, 1, 1, 1, 0, 0, 1, 0],\n",
       "         [0, 0, 1, 0, 1, 1, 1, 0, 0, 0],\n",
       "         [0, 1, 1, 0, 0, 0, 1, 0, 0, 1],\n",
       "         [1, 0, 1, 0, 0, 0, 1, 1, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 1, 0]]),\n",
       " 'matrix': tensor([[-1.4367e+00, -1.1598e-01, -1.9689e-01, -5.7622e-03, -3.5927e-02,\n",
       "          -8.1172e-02, -1.0770e+00, -7.6900e-02, -9.7782e-01, -1.0165e+00],\n",
       "         [-4.7375e+00, -4.0627e-01, -2.8540e-01, -1.1106e+00, -6.2159e-01,\n",
       "          -1.5948e+00, -3.5791e-03, -1.5757e+00, -1.1874e-04, -3.9450e+00],\n",
       "         [-8.8832e-01, -3.5602e-01, -4.8977e-01, -3.2474e-02, -1.9861e-01,\n",
       "          -8.2896e-04, -1.6742e+00, -4.4914e-04, -1.5499e+00, -5.6569e-01],\n",
       "         [-6.0976e-01, -5.7504e-01, -7.4213e-01, -1.1686e-01, -3.6881e-01,\n",
       "          -1.7648e-02, -2.1186e+00, -1.9724e-02, -1.9785e+00, -3.4868e-01],\n",
       "         [-1.1803e+00, -2.0500e-01, -3.0906e-01, -1.3177e-03, -9.1055e-02,\n",
       "          -2.9824e-02, -1.3225e+00, -2.7258e-02, -1.2123e+00, -8.0287e-01],\n",
       "         [-3.2619e+00, -7.1223e-02, -2.6804e-02, -4.6697e-01, -1.7464e-01,\n",
       "          -7.9628e-01, -1.8519e-01, -7.8278e-01, -1.4547e-01, -2.6104e+00],\n",
       "         [-4.3030e+00, -2.8642e-01, -1.8665e-01, -9.0565e-01, -4.7088e-01,\n",
       "          -1.3471e+00, -2.6253e-02, -1.3295e+00, -1.2792e-02, -3.5494e+00],\n",
       "         [-7.2361e-01, -4.7407e-01, -6.2677e-01, -7.4015e-02, -2.8892e-01,\n",
       "          -3.9765e-03, -1.9203e+00, -4.9925e-03, -1.7871e+00, -4.3596e-01],\n",
       "         [-8.1595e+00, -1.7353e+00, -1.4742e+00, -3.0060e+00, -2.1560e+00,\n",
       "          -3.7744e+00, -3.8451e-01, -3.7449e+00, -4.4759e-01, -7.1081e+00],\n",
       "         [-3.0734e-01, -9.6982e-01, -1.1836e+00, -3.2299e-01, -6.9518e-01,\n",
       "          -1.2912e-01, -2.8292e+00, -1.3463e-01, -2.6670e+00, -1.3250e-01],\n",
       "         [-1.4790e+00, -1.0435e-01, -1.8163e-01, -8.7320e-03, -2.9587e-02,\n",
       "          -9.1472e-02, -1.0409e+00, -8.6933e-02, -9.4345e-01, -1.0522e+00],\n",
       "         [-9.5473e+00, -2.4046e+00, -2.0953e+00, -3.8697e+00, -2.8958e+00,\n",
       "          -4.7357e+00, -7.2842e-01, -4.7027e+00, -8.1433e-01, -8.4071e+00],\n",
       "         [-5.0487e+00, -5.0090e-01, -3.6553e-01, -1.2639e+00, -7.3748e-01,\n",
       "          -1.7775e+00, -1.1088e-04, -1.7573e+00, -3.5353e-03, -4.2294e+00],\n",
       "         [-6.9340e+00, -1.1970e+00, -9.8190e-01, -2.2817e+00, -1.5502e+00,\n",
       "          -2.9568e+00, -1.5749e-01, -2.9307e+00, -1.9872e-01, -5.9676e+00],\n",
       "         [-9.3033e+00, -2.2830e+00, -1.9819e+00, -3.7150e+00, -2.7622e+00,\n",
       "          -4.5643e+00, -6.6217e-01, -4.5319e+00, -7.4420e-01, -8.1782e+00],\n",
       "         [-6.1352e-01, -5.7140e-01, -7.3799e-01, -1.1522e-01, -3.6589e-01,\n",
       "          -1.7014e-02, -2.1116e+00, -1.9054e-02, -1.9718e+00, -3.5152e-01]]),\n",
       " 'lse_loss': tensor([0.2544])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent = Latent(init_log_sigma=0, const_sigma=True)\n",
    "data = {'z': torch.randn(4, 1, 2, 2),\n",
    "        'e': torch.randn(10, 1),\n",
    "        'belong': torch.randint(0, 2, size=(4*2*2, 10))}\n",
    "latent(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76615163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 0, 1]])\n",
      "tensor([[ 0.9080,  1.1270, -1.4178, -0.1093],\n",
      "        [-0.4860,  0.5325, -0.0897,  0.7186],\n",
      "        [ 0.9674, -1.4297, -0.4424, -0.6589]], requires_grad=True)\n",
      "tensor([[  0.9080, -33.4118, -35.9566,  -0.1093],\n",
      "        [ -0.4860,   0.5325,  -0.0897,   0.7186],\n",
      "        [  0.9674,  -1.4297, -34.9812,  -0.6589]])\n",
      "tensor([[4.3304e-01, 1.5888e-15, 2.6498e-16, 2.5869e-01],\n",
      "        [1.0743e-01, 8.7677e-01, 1.0000e+00, 5.9200e-01],\n",
      "        [4.5953e-01, 1.2323e-01, 7.0277e-16, 1.4931e-01]])\n"
     ]
    }
   ],
   "source": [
    "# 함수를 사용하려면 apply 메서드를 사용하고, dim 매개변수를 전달합니다.\n",
    "input = torch.randn(3, 4, requires_grad=True)\n",
    "belong = torch.randint(0, 2, size=[*input.shape])\n",
    "output = CustomLogSumExp.apply(input, belong, 0)\n",
    "output.backward(torch.ones_like(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36128b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7e99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a18a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ee3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
