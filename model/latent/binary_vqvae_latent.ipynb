{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cf5ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Bottle(nn.Module):\n",
    "    def __init__(self, K, latent_channels):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.latent_channels = latent_channels\n",
    "        self.codebook_sum = None\n",
    "        self.codebook_elem = None\n",
    "        self.register_buffer('codebook', torch.zeros(self.K, self.latent_channels))\n",
    "        self.threshold = 1.0\n",
    "        self.mu = 0.99\n",
    "        self.register_buffer('init', torch.zeros(1))\n",
    "                \n",
    "    def _quantize(self, ze):\n",
    "        # ze : (n, c)\n",
    "        \n",
    "        # (n, K)\n",
    "        distance = (ze**2).sum(-1, keepdim=True) -\\\n",
    "                   2*ze@self.codebook.T +\\\n",
    "                   (self.codebook.T**2).sum(0, keepdim=True)\n",
    "        # (n,), (n,)\n",
    "        min_distance, zi = torch.min(distance, dim=-1)\n",
    "        return zi\n",
    "    \n",
    "    def _dequantize(self, zi):\n",
    "        # zi : (n,)\n",
    "        # (n, c)\n",
    "        zq = F.embedding(zi, self.codebook)\n",
    "        return zq\n",
    "    \n",
    "    def _tile(self, x):\n",
    "        d, ew = x.shape\n",
    "        if d < self.k_bins:\n",
    "            n_repeats = (self.k_bins + d - 1) // d\n",
    "            std = 0.01 / np.sqrt(ew)\n",
    "            x = x.repeat(n_repeats, 1)\n",
    "            x = x + t.randn_like(x) * std\n",
    "        return x\n",
    "    \n",
    "    # Choose K vectors from the data with additional noise\n",
    "    def _get_codebook_from_data(self, ze):\n",
    "        # ze: (K, c)\n",
    "        \n",
    "        codebooks = []\n",
    "        k = 0\n",
    "        while True:\n",
    "            codebook = ze[torch.randperm(ze.shape[0])][:self.K]\n",
    "            std = 0.01 / np.sqrt(self.latent_channels)\n",
    "            codebook = codebook + torch.randn_like(codebook) * std\n",
    "            codebooks.append(codebook)\n",
    "            k += len(codebook)\n",
    "            if k >= self.K:\n",
    "                break\n",
    "        codebook = torch.cat(codebooks, dim=0)[:self.K]\n",
    "        return codebook\n",
    "    \n",
    "    def _initialize(self, ze):\n",
    "        # ze : (n, c)\n",
    "        \n",
    "        self.codebook = self._get_codebook_from_data(ze)\n",
    "        self.codebook_sum = self.codebook.clone()\n",
    "        self.codebook_elem = torch.ones(self.K, device=self.codebook.device)\n",
    "        \n",
    "    def _update(self, ze, zi):\n",
    "        # ze : (n, c)\n",
    "        # zi : (n,)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            '''Calculate current centroids of the z embeddings = codebook_sum/codebook_elem'''\n",
    "            # (n, K)\n",
    "            zi_onehot = F.one_hot(zi, num_classes=self.K).float()\n",
    "            # (K, c) = (K, n) @ (n, c)\n",
    "            codebook_sum_current = zi_onehot.T @ ze\n",
    "            # (K,)\n",
    "            codebook_elem_current = zi_onehot.sum(0)\n",
    "\n",
    "            '''Obtain randomly a new centroids for bins whose usage is lower than the threshold'''\n",
    "            # (K, c)\n",
    "            codebook_random = self._get_codebook_from_data(ze)\n",
    "            \n",
    "            if self.init and self.codebook_sum is None:\n",
    "                self.codebook_sum = self.codebook.clone()\n",
    "                self.codebook_elem = torch.ones(self.K, device=self.codebook.device)\n",
    "\n",
    "            '''Update current centroids parameters'''\n",
    "            self.codebook_sum = self.mu*self.codebook_sum + (1.-self.mu)*codebook_sum_current\n",
    "            self.codebook_elem = self.mu*self.codebook_elem + (1.-self.mu)*codebook_elem_current\n",
    "\n",
    "            '''Update centroids'''\n",
    "            # (K, 1)\n",
    "            usage = (self.codebook_elem.unsqueeze(1) >= self.threshold).float()\n",
    "            codebook_prob = self.codebook_elem / self.codebook_elem.sum()\n",
    "            entropy = -torch.sum(codebook_prob*torch.log(codebook_prob + 1e-8))\n",
    "            # (K, c)\n",
    "            codebook_new = self.codebook_sum / self.codebook_elem.unsqueeze(1)\n",
    "            # (k, c)\n",
    "            self.codebook = usage*codebook_new + (1-usage)*codebook_random\n",
    "            outputs = {'usage': usage.sum(),\n",
    "                       'entropy': entropy}\n",
    "            return outputs\n",
    "        \n",
    "    def forward(self, ze, q_level=1.0):\n",
    "        # ze : (b, c)\n",
    "                \n",
    "        if not self.init:\n",
    "            self.init.data.fill_(1.)\n",
    "            self._initialize(ze)\n",
    "                        \n",
    "        # z_index : (b,)\n",
    "        zi = self._quantize(ze)\n",
    "        # z_quantized : (b, c)\n",
    "        zq = self._dequantize(zi)\n",
    "        # update codebook\n",
    "        update_outputs = self._update(ze, zi)\n",
    "        # Commitment loss\n",
    "        commit_loss = F.mse_loss(ze, zq)\n",
    "        # pass-through\n",
    "        zq = ze + (zq - ze).detach() * q_level\n",
    "        outputs = {'commit_loss': commit_loss,\n",
    "                   'zi': zi,\n",
    "                   'zq': zq}\n",
    "        outputs.update(update_outputs)\n",
    "        return outputs\n",
    "    \n",
    "class Latent(nn.Module):\n",
    "    def __init__(self, n_latents, z_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.bottles = nn.ModuleList([Bottle(2, z_dim) for _ in range(n_latents)])\n",
    "        self.linear = nn.Linear(z_dim, n_latents*z_dim)\n",
    "                \n",
    "    def forward(self, data, **kwargs):\n",
    "        # data['z'] : (N, z, H, W)\n",
    "        \n",
    "        N, z_dim, H, W = data['z'].size()\n",
    "        # (NHW, z)\n",
    "        z = data['z'].permute(0, 2, 3, 1).reshape(N*H*W, z_dim)\n",
    "        # (BHW, L, z)\n",
    "        z = self.linear(z).reshape(-1, len(self.bottles), z_dim)\n",
    "        commit_loss = 0\n",
    "        zq = []\n",
    "        for l in range(len(self.bottles)):\n",
    "            outputs = self.bottles[l](z[:, l])\n",
    "            commit_loss += outputs['commit_loss']\n",
    "            zq.append(outputs['zq'])\n",
    "        data['commit_loss'] = commit_loss\n",
    "        # (NHW, z_dim)\n",
    "        zq = torch.sum(torch.stack(zq, dim=0), dim=0)\n",
    "        # (N, z, H, W)\n",
    "        data['z'] = zq.reshape(N, H, W, z_dim).permute(0, 3, 1, 2)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "232fcf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['z', 'commit_loss'])\n"
     ]
    }
   ],
   "source": [
    "data = {'z': torch.randn(2, 512, 8, 8)}\n",
    "data = Latent(9, 512)(data)\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7745b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad4fe59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21af0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
