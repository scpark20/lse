{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1dbf42f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3818097596.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    z_proj = data['z'] @\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Latent(nn.Module):\n",
    "    def __init__(self, init_log_sigma, **kwargs):\n",
    "        super().__init__()\n",
    "        self.log_sigma = nn.Parameter(torch.ones(1) * init_log_sigma)\n",
    "                \n",
    "    def forward(self, data, **kwargs):\n",
    "        # data['z'] : (N, c)\n",
    "        # data['e'] : (M, c)\n",
    "        \n",
    "        N = len(data['z'])\n",
    "        z_dim = data['z'].shape[1]\n",
    "        \n",
    "        # (50, z)\n",
    "        projections = rand_projections(z_dim, data['z'].device)\n",
    "        # (N, 50)\n",
    "        z_proj = data['z'] @ projections.T\n",
    "        # (M, 50)\n",
    "        e_proj = data['e'] @ projections.T\n",
    "        \n",
    "        distance = torch.sort(z_proj.T, dim=1)[0] - torch.sort(e_proj.T, dim=1)[0]\n",
    "        distance = torch.abs(distance) ** 2\n",
    "        data['swae_loss'] = torch.mean(distance)\n",
    "\n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c38d8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_projections(embedding_dim, num_samples=50, device='cpu'):\n",
    "    \"\"\"This function generates `num_samples` random samples from the latent space's unit sphere.\n",
    "        Args:\n",
    "            embedding_dim (int): embedding dimensionality\n",
    "            num_samples (int): number of random projection samples\n",
    "        Return:\n",
    "            torch.Tensor: tensor of size (num_samples, embedding_dim)\n",
    "    \"\"\"\n",
    "    unnormd = torch.randn(num_samples, embedding_dim, device=device)\n",
    "\n",
    "    projections = unnormd.div( torch.norm(unnormd,dim=1,keepdim=True) )\n",
    "    return projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baf3af27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_projections(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f239a545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
