{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d720d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Latent(nn.Module):\n",
    "    def __init__(self, init_log_sigma, const_sigma, **kwargs):\n",
    "        super().__init__()\n",
    "        self.log_sigma = nn.Parameter(torch.ones(1) * init_log_sigma, requires_grad=not const_sigma)\n",
    "                \n",
    "    def forward(self, data, **kwargs):\n",
    "        # data['z'] : (N, c, H, W)\n",
    "        # data['e'] : (M, c)\n",
    "        \n",
    "        z_dim = data['z'].shape[1]\n",
    "        # (NHW, c)\n",
    "        z = data['z'].permute(0, 2, 3, 1).reshape(-1, z_dim)\n",
    "        N = len(z)\n",
    "        T = kwargs['latent_temp'] if 'latent_temp' in kwargs else 1.0\n",
    "        softmax_temp = kwargs['softmax_temp'] if 'softmax_temp' in kwargs else 1.0\n",
    "        \n",
    "        # (NHW, M) = sum((NHW, 1, z) - (1, M, z), dim=2)\n",
    "        distance = torch.norm(z.unsqueeze(1) - data['e'].unsqueeze(0), dim=2) ** 2\n",
    "        alpha = -1/(2*torch.exp(self.log_sigma)**2)\n",
    "        matrix = alpha*distance/T\n",
    "        data['matrix'] = matrix\n",
    "        # (NHW, M)\n",
    "        belong = data['belong'] if 'belong' in data else None\n",
    "        loss = -torch.mean(T*CustomLogSumExp.apply(matrix, belong, softmax_temp))\n",
    "        loss = loss + 0.5*z_dim*(2*self.log_sigma-np.log(np.e)) + np.log(N)        \n",
    "        data['lse_loss'] = loss\n",
    "        \n",
    "        return data\n",
    "    \n",
    "class CustomLogSumExp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, belong=None, temp=1):\n",
    "        # input : (N, M)\n",
    "        # belong : (N, M)\n",
    "        \n",
    "        ctx.temp = temp\n",
    "        # (1, M)\n",
    "        output = torch.logsumexp(input, dim=0, keepdim=True)\n",
    "        ctx.save_for_backward(input, output, belong)\n",
    "        return output.squeeze(0)  # output을 반환할 때는 차원을 줄입니다.\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        temp = ctx.temp\n",
    "        # (N, M), (1, M), (N, M)\n",
    "        input, output, belong = ctx.saved_tensors\n",
    "        # softmax 함수를 사용하여 그래디언트 계산을 수행합니다.\n",
    "        # (N, M)\n",
    "        if belong is None:\n",
    "            softmax_result = torch.exp(input - output)\n",
    "        else:\n",
    "            inner_value = belong * input + (1-belong) * (np.log(max(temp, 1e-15)) + input)\n",
    "            softmax_result = torch.softmax(inner_value, dim=0)\n",
    "        grad_input = softmax_result * grad_output.unsqueeze(0)\n",
    "        return grad_input, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3809e700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'z': tensor([[[[-1.3160, -0.2128],\n",
       "           [-1.0209, -1.3800]]],\n",
       " \n",
       " \n",
       "         [[[-0.7644,  0.7662],\n",
       "           [ 0.5318,  1.7991]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2352,  0.9730],\n",
       "           [-1.6667,  0.6466]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0054, -0.7841],\n",
       "           [-2.0037, -0.5783]]]]),\n",
       " 'e': tensor([[ 0.6101],\n",
       "         [-1.0073],\n",
       "         [ 0.5925],\n",
       "         [-0.6866],\n",
       "         [ 0.0954],\n",
       "         [ 1.2205],\n",
       "         [-1.6139],\n",
       "         [ 0.0961],\n",
       "         [ 0.9006],\n",
       "         [ 0.3560]]),\n",
       " 'belong': tensor([[1, 0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
       "         [0, 1, 1, 1, 1, 0, 0, 1, 1, 1],\n",
       "         [1, 0, 1, 1, 1, 1, 1, 0, 1, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 1, 0, 1, 1, 0, 0, 1, 1],\n",
       "         [1, 1, 1, 0, 1, 0, 1, 0, 0, 1],\n",
       "         [0, 1, 1, 0, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "         [1, 0, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "         [1, 0, 1, 1, 1, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
       "         [1, 1, 0, 1, 0, 1, 0, 1, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       "         [0, 1, 0, 1, 1, 1, 0, 1, 1, 1]]),\n",
       " 'matrix': tensor([[-1.8548e+00, -4.7644e-02, -1.8211e+00, -1.9803e-01, -9.9594e-01,\n",
       "          -3.2168e+00, -4.4388e-02, -9.9696e-01, -2.4566e+00, -1.3978e+00],\n",
       "         [-3.3860e-01, -3.1557e-01, -3.2428e-01, -1.1224e-01, -4.7497e-02,\n",
       "          -1.0272e+00, -9.8151e-01, -4.7720e-02, -6.1989e-01, -1.6179e-01],\n",
       "         [-1.3300e+00, -9.2227e-05, -1.3015e+00, -5.5853e-02, -6.2299e-01,\n",
       "          -2.5119e+00, -1.7586e-01, -6.2380e-01, -1.8460e+00, -9.4788e-01],\n",
       "         [-1.9803e+00, -6.9478e-02, -1.9455e+00, -2.4041e-01, -1.0884e+00,\n",
       "          -3.3814e+00, -2.7348e-02, -1.0895e+00, -2.6007e+00, -1.5069e+00],\n",
       "         [-9.4466e-01, -2.9485e-02, -9.2064e-01, -3.0269e-03, -3.6964e-01,\n",
       "          -1.9700e+00, -3.6081e-01, -3.7026e-01, -1.3862e+00, -6.2770e-01],\n",
       "         [-1.2187e-02, -1.5726e+00, -1.5088e-02, -1.0554e+00, -2.2501e-01,\n",
       "          -1.0319e-01, -2.8325e+00, -2.2453e-01, -9.0322e-03, -8.4133e-02],\n",
       "         [-3.0665e-03, -1.1843e+00, -1.8438e-03, -7.4226e-01, -9.5222e-02,\n",
       "          -2.3718e-01, -2.3020e+00, -9.4907e-02, -6.8022e-02, -1.5447e-02],\n",
       "         [-7.0690e-01, -3.9379e+00, -7.2796e-01, -3.0895e+00, -1.4514e+00,\n",
       "          -1.6740e-01, -5.8244e+00, -1.4501e+00, -4.0365e-01, -1.0413e+00],\n",
       "         [-7.0275e-02, -7.7186e-01, -6.3836e-02, -4.2488e-01, -9.7738e-03,\n",
       "          -4.8543e-01, -1.7096e+00, -9.6730e-03, -2.2140e-01, -7.2988e-03],\n",
       "         [-6.5842e-02, -1.9607e+00, -7.2379e-02, -1.3771e+00, -3.8509e-01,\n",
       "          -3.0637e-02, -3.3460e+00, -3.8445e-01, -2.6176e-03, -1.9032e-01],\n",
       "         [-2.5919e+00, -2.1744e-01, -2.5521e+00, -4.8029e-01, -1.5525e+00,\n",
       "          -4.1681e+00, -1.3945e-03, -1.5538e+00, -3.2956e+00, -2.0457e+00],\n",
       "         [-6.6552e-04, -1.3676e+00, -1.4618e-03, -8.8872e-01, -1.5191e-01,\n",
       "          -1.6470e-01, -2.5549e+00, -1.5151e-01, -3.2269e-02, -4.2213e-02],\n",
       "         [-1.8282e-01, -5.1276e-01, -1.7234e-01, -2.3946e-01, -4.0473e-03,\n",
       "          -7.3824e-01, -1.3111e+00, -4.1126e-03, -4.0070e-01, -6.1462e-02],\n",
       "         [-9.7187e-01, -2.4906e-02, -9.4750e-01, -4.7490e-03, -3.8673e-01,\n",
       "          -2.0092e+00, -3.4431e-01, -3.8737e-01, -1.4191e+00, -6.4991e-01],\n",
       "         [-3.4161e+00, -4.9648e-01, -3.3703e+00, -8.6740e-01, -2.2032e+00,\n",
       "          -5.1979e+00, -7.5986e-02, -2.2047e+00, -4.2177e+00, -2.7842e+00],\n",
       "         [-7.0612e-01, -9.2016e-02, -6.8537e-01, -5.8694e-03, -2.2691e-01,\n",
       "          -1.6178e+00, -5.3626e-01, -2.2740e-01, -1.0936e+00, -4.3646e-01]]),\n",
       " 'lse_loss': tensor([0.0868])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent = Latent(init_log_sigma=0, const_sigma=True)\n",
    "data = {'z': torch.randn(4, 1, 2, 2),\n",
    "        'e': torch.randn(10, 1),\n",
    "        'belong': torch.randint(0, 2, size=(4*2*2, 10))}\n",
    "latent(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "76615163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수를 사용하려면 apply 메서드를 사용하고, dim 매개변수를 전달합니다.\n",
    "input = torch.randn(3, 4, requires_grad=True)\n",
    "belong = torch.randint(0, 2, size=[*input.shape])\n",
    "output = CustomLogSumExp.apply(input, belong, 0)\n",
    "output.backward(torch.ones_like(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36128b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7e99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a18a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54324897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
